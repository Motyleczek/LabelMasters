{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obliczanie miar dla wyników detekcji systemu\n",
    "\n",
    "W tym notebooku zarówno obliczane były finalne wyniki WER, CER oraz MER, ale i tutaj zapisane były przykładowe detekcje do pokazania w analizie wyników w pracy magisterskiej. \n",
    "\n",
    "Wyniki uzyskane z dwóch ostatnich komórek tego notebooka zostały zaprezentowane w analizie wyników pracy magisterskiej. \n",
    "\n",
    "Część tego notebooka (zaznaczona odpowiednio) była przeliczona na środowisku Google Colabolatory ze względu na inną architekturę mojej lokalnej maszyny w porównaniu do środowiska w którym uczony był model. Niemniej jednak finalne obliczenia na zapisanych detekcjach zostały przeprowadzone lokalnie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importy oraz przygotowanie pomocnych funkcji/narzędzi\n",
    "\n",
    "import jiwer\n",
    "import pytesseract\n",
    "import cv2\n",
    "import os \n",
    "import numpy as np\n",
    "import pickle\n",
    "import io\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# funkcje do  preprocessingu obrazów przed detekcjami tesseract\n",
    "def get_grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def thresholding(image):\n",
    "    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "# transformacje potrzebne do poprawnego obliczania WER/MER/CER \n",
    "transforms = jiwer.Compose(\n",
    "    [\n",
    "        jiwer.RemoveEmptyStrings(),\n",
    "        jiwer.ToLowerCase(),\n",
    "        jiwer.RemoveMultipleSpaces(),\n",
    "        jiwer.Strip(),\n",
    "        jiwer.RemovePunctuation(),\n",
    "        jiwer.ReduceToListOfListOfWords(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Przeliczone na collabolatory:** komórka bezpośrednio pod spodem została przeliczona w tym samym środowisku co trenowanie modelu. Po obliczeniach wyniki zostały zapisane z pomocą biblioteki `pickle` a następnie przeliczone lokalnie (kolejne komórki)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# całość poniej była puszczona na colabie, ze względu na błędy modelu kiedy puszczałem go lokalnie\n",
    "# tak pozyskałem wyniki (results) dla zbiorów treningowego, validacyjnego oraz testowego\n",
    "# te wyniki uzywam ponizej razem z biblioteką jitter do obliczenia wynikow WER\n",
    "\n",
    "######################\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "\n",
    "# print(\"loading model\")\n",
    "# model = YOLO(\"ran_on_colab/weights/best_best.pt\")\n",
    "# print(\"loaded model\")\n",
    "\n",
    "# path_to_train_im = \"ran_on_colab/images/test/\"\n",
    "# path_to_train_info_excel = \"data/generated_labels/first_gen_2024-06-24.xlsx\"\n",
    "\n",
    "# files_np = np.array(os.listdir(path_to_train_im))\n",
    "# file_paths = np.char.add(path_to_train_im, files_np)\n",
    "\n",
    "# file_paths_short = file_paths[:20]\n",
    "\n",
    "# results = model(source=file_paths_short.tolist())\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ładowanie plików pickle zwróconych przez bibliotekę torch musi zostać przeprowadzone w specyficzny sposób ze wzgledu na inne architektury sprzętu na którym prowadzona była nauka modelu (GPU) a na którym liczone są miary (CPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ze wzgledu na roznice maszyn potrzeba otworzyć w specjlany sposob:\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else:\n",
    "            return super().find_class(module, name)\n",
    "\n",
    "with open(\"ran_on_colab/gdrive_structure/results_saved/train/train_results.pkl\", \"rb\") as file:\n",
    "    loaded_train_results = CPU_Unpickler(file).load()\n",
    "\n",
    "with open(\"ran_on_colab/gdrive_structure/results_saved/test/test_results.pkl\", \"rb\") as file:\n",
    "    loaded_test_results = CPU_Unpickler(file).load()\n",
    "    \n",
    "with open(\"ran_on_colab/gdrive_structure/results_saved/val/val_results.pkl\", \"rb\") as file:\n",
    "    loaded_val_results = CPU_Unpickler(file).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label_tag</th>\n",
       "      <th>label_type</th>\n",
       "      <th>uwagi</th>\n",
       "      <th>odbiorca</th>\n",
       "      <th>nadawca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dhl_1</td>\n",
       "      <td>Proszę zadzwonić domofonem.</td>\n",
       "      <td>Magdalena Kozak ul. Michałowska 113 03-767 W...</td>\n",
       "      <td>Mateusz Stępień ul. Nawigacyjna 19 60-480 Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>inpost_3</td>\n",
       "      <td>Upewnić się, że ktoś odbierze</td>\n",
       "      <td>MORSKI KWIAT   ul. Wałuszewska 55 03-005 Warsz...</td>\n",
       "      <td>SŁONECZNA POLANA   ul. Obrzańska 86 60-114 Poz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>inpost_3</td>\n",
       "      <td>Zadzwonić na numer kontaktowy</td>\n",
       "      <td>Gabriela Pawlik ul. Mickiewicza Adama 113 81...</td>\n",
       "      <td>Maria Nowak ul. Generała Fiszera Józefa 20 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>dhl_1</td>\n",
       "      <td>Proszę o delikatne obchodzeni</td>\n",
       "      <td>Mariusz Piekarski ul. Mała 144 15-317 Białys...</td>\n",
       "      <td>Krzysztof Kwiatkowski ul. Brzozowa 125 43-60...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>pocztex_1</td>\n",
       "      <td>Proszę dostarczyć na tylną fu</td>\n",
       "      <td>Paulina Łukasik ul. Wierzbowa 64 41-100 Siem...</td>\n",
       "      <td>Joanna Borowska ul. Płońska 48 03-683 Warsza...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label_tag label_type                          uwagi  \\\n",
       "0           0          0      dhl_1    Proszę zadzwonić domofonem.   \n",
       "1           0          1   inpost_3  Upewnić się, że ktoś odbierze   \n",
       "2           0          2   inpost_3  Zadzwonić na numer kontaktowy   \n",
       "3           0          3      dhl_1  Proszę o delikatne obchodzeni   \n",
       "4           0          4  pocztex_1  Proszę dostarczyć na tylną fu   \n",
       "\n",
       "                                            odbiorca  \\\n",
       "0    Magdalena Kozak ul. Michałowska 113 03-767 W...   \n",
       "1  MORSKI KWIAT   ul. Wałuszewska 55 03-005 Warsz...   \n",
       "2    Gabriela Pawlik ul. Mickiewicza Adama 113 81...   \n",
       "3    Mariusz Piekarski ul. Mała 144 15-317 Białys...   \n",
       "4    Paulina Łukasik ul. Wierzbowa 64 41-100 Siem...   \n",
       "\n",
       "                                             nadawca  \n",
       "0    Mateusz Stępień ul. Nawigacyjna 19 60-480 Po...  \n",
       "1  SŁONECZNA POLANA   ul. Obrzańska 86 60-114 Poz...  \n",
       "2    Maria Nowak ul. Generała Fiszera Józefa 20 8...  \n",
       "3    Krzysztof Kwiatkowski ul. Brzozowa 125 43-60...  \n",
       "4    Joanna Borowska ul. Płońska 48 03-683 Warsza...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ładowanie zawartości wygenerowanych etykiet z pliku utworzonego podczas generacji\n",
    "# detekcja będzie porównywana do generacji, na tej podstawie zostaną obliczone miary detekcji\n",
    "\n",
    "path_to_train_info_excel = \"data/generated_labels/first_gen_2024-06-24.xlsx\"\n",
    "generated_df = pd.read_excel(path_to_train_info_excel)\n",
    "\n",
    "generated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyliczenia miar\n",
    "\n",
    "W następnych dwóch komórkach obliczane są miary dla wyników detekcji na zbiorze treningowym oraz walidacyjnym (odpowiednio opisane komentarzem). Dodatkowo w trakcie obliczeń zapisywane są przykładowe detekcje w celach pokazowych (obecnie zakomentowana linijka). Finalnie przez odkomentowanie odpowiedniej linijki (opisana komentarzem) da się spojrzeć na pojedyńcze detekcje w porównaniu z targetem (wygenerowanymi danymi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miary dla detekcji zbioru treningowego, z podziałem na typ informacji: \n",
      "\n",
      "  kategoria  WER mean   WER std  Max WER mean  Max WER std\n",
      "0     Uwagi  0.875541  0.342468      1.630519     0.283154\n",
      "1  Odbiorca  0.888041  0.250870      1.142028     0.229027\n",
      "2   Nadawca  0.869868  0.133691      1.063807     0.106329 \n",
      "\n",
      "  kategoria  CER mean   CER std  Max CER mean  Max CER std\n",
      "0     Uwagi  0.875541  0.342468      1.404144     0.250582\n",
      "1  Odbiorca  0.888041  0.250870      1.047145     0.178414\n",
      "2   Nadawca  0.869868  0.133691      1.026332     0.051319 \n",
      "\n",
      "  kategoria  MER mean   MER std\n",
      "0     Uwagi  0.685962  0.185165\n",
      "1  Odbiorca  0.816855  0.139853\n",
      "2   Nadawca  0.847498  0.132280 \n",
      "\n",
      "\n",
      "Miary dla detekcji zbioru treningowego, z podziałem na typ przewoźnika:\n",
      "  przewoznik  WER mean   WER std  Max WER mean  Max WER std\n",
      "0     inpost  0.877803  0.254390      1.317402     0.374447\n",
      "1    pocztex  0.873175  0.243198      1.284329     0.313042\n",
      "2        dpd  0.866352  0.476239      1.291502     0.337048\n",
      "3        dhl  0.889000  0.155359      1.204788     0.209350 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obliczenia dla zbioru treningowego\n",
    "from pytesseract import Output\n",
    "\n",
    "class_names_dict = {0:\"nadawca\",\n",
    "                    1:\"odbiorca\",\n",
    "                    2:\"uwagi\"}\n",
    "\n",
    "# będą to lity dwuelementowych list, pierwszy element to wartość, drugi to wartość maksymalna\n",
    "nadawca_scores = []\n",
    "odbiorca_scores = []\n",
    "uwagi_scores = []\n",
    "\n",
    "nadawca_scores_cer = []\n",
    "odbiorca_scores_cer = []\n",
    "uwagi_scores_cer = []\n",
    "\n",
    "nadawca_scores_mer = []\n",
    "odbiorca_scores_mer = []\n",
    "uwagi_scores_mer = []\n",
    "\n",
    "inpost_wer = []\n",
    "dhl_wer = []\n",
    "dpd_wer = []\n",
    "pocztex_wer = []\n",
    "\n",
    "\n",
    "for k, result in enumerate(loaded_train_results):\n",
    "    orig_image = result.orig_img\n",
    "    fullpath = result.path\n",
    "    number = int(fullpath.split(\"_\")[-1][:-5]) # zadziała dla train i val, nie dla test\n",
    "    detected_classes = result.boxes.cls\n",
    "    detected_boxes = result.boxes\n",
    "    \n",
    "    for i, class_detected in enumerate(detected_classes):\n",
    "        class_int = int(class_detected.numpy())\n",
    "        [x1, y1, x2, y2] = detected_boxes[i].xyxy.numpy().astype(int)[0]\n",
    "        fragment = orig_image[y1:y2, x1:x2, :] # działa! \n",
    "        \n",
    "        # #blur\n",
    "        # img = cv2.blur(fragment, (10,10))\n",
    "        \n",
    "        # #sharpen\n",
    "        # kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "        # img = cv2.filter2D(img, -1, kernel)\n",
    "        \n",
    "        img = get_grayscale(fragment)\n",
    "        img = cv2.GaussianBlur(img,(5,5),0)\n",
    "        # img = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "        #     cv2.THRESH_BINARY,11,2)\n",
    "        img = thresholding(img)\n",
    "        \n",
    "        #open\n",
    "        kernel = np.ones((4,4),np.uint8)\n",
    "        img = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        #close\n",
    "        kernel = np.ones((4,4),np.uint8)\n",
    "        img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        \n",
    "        text_on_fragment = pytesseract.image_to_string(img)\n",
    "\n",
    "        text_category = class_names_dict[class_int]\n",
    "        text_to_decipher = generated_df.iloc[number][text_category]\n",
    "        label_type = generated_df.iloc[number]['label_type'].split('_')[0]\n",
    "        \n",
    "        \n",
    "        N_1 = len(re.split(r'\\s+', text_to_decipher))\n",
    "        N_2 = len(re.split(r'\\s+', text_on_fragment))\n",
    "        \n",
    "        max_wer = max(N_1, N_2)/N_1\n",
    "        \n",
    "        \n",
    "        wer = jiwer.wer(\n",
    "                text_to_decipher,\n",
    "                text_on_fragment,\n",
    "                truth_transform=transforms,\n",
    "                hypothesis_transform=transforms,\n",
    "            )\n",
    "\n",
    "        cleaned_str_ref = text_to_decipher.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        cleaned_str_hip = text_on_fragment.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        N_1 = len(cleaned_str_ref)\n",
    "        N_2 = len(cleaned_str_hip)\n",
    "        \n",
    "        max_cer = max(N_1, N_2)/N_1\n",
    "        \n",
    "        \n",
    "        cer = jiwer.cer(\n",
    "                text_to_decipher,\n",
    "                text_on_fragment,\n",
    "                truth_transform=transforms,\n",
    "                hypothesis_transform=transforms,\n",
    "            )\n",
    "        \n",
    "        mer = jiwer.mer(\n",
    "                text_to_decipher,\n",
    "                text_on_fragment,\n",
    "                truth_transform=transforms,\n",
    "                hypothesis_transform=transforms,\n",
    "            )\n",
    "        \n",
    "        \n",
    "        if label_type == \"inpost\":\n",
    "            inpost_wer.append([wer, max_wer])\n",
    "        elif label_type == \"dhl\":\n",
    "            dhl_wer.append([wer, max_wer])\n",
    "        elif label_type == \"pocztex\":\n",
    "            pocztex_wer.append([wer, max_wer])\n",
    "        else:\n",
    "            dpd_wer.append([wer, max_wer])\n",
    "        \n",
    "        if class_int == 0:\n",
    "            nadawca_scores.append([wer, max_wer])\n",
    "            nadawca_scores_cer.append([cer, max_cer])\n",
    "            nadawca_scores_mer.append(mer)\n",
    "        elif class_int == 1:\n",
    "            odbiorca_scores.append([wer, max_wer])\n",
    "            odbiorca_scores_cer.append([cer, max_cer])\n",
    "            odbiorca_scores_mer.append(mer)\n",
    "        elif class_int == 2:\n",
    "            uwagi_scores.append([wer, max_wer])\n",
    "            uwagi_scores_cer.append([cer, max_cer])\n",
    "            uwagi_scores_mer.append(mer)\n",
    "        \n",
    "        # cały fragment w dół jedynie dla wglądu w wyniki w razie potrzeby\n",
    "        # if k in [4, 15, 17]:\n",
    "            \n",
    "        #     # dla wglądu w to co model wykrywa (tekst) i porównania co powinno wykryć, odkomentować ponizsza linijke\n",
    "        #     #####\n",
    "        #     # print(\"Co wykryło: \\n ######### \\n\", text_on_fragment, \"\\n #########, co miało być: ######### \\n\", text_to_decipher, \"\\n\\n\")\n",
    "        #     #####\n",
    "            \n",
    "        #     # umieszczanie bounding boxów na obrazach\n",
    "        #     d = pytesseract.image_to_data(img, output_type=Output.DICT)\n",
    "        #     n_boxes = len(d['level'])\n",
    "        #     for j in range(n_boxes):\n",
    "        #         (x, y, w, h) = (d['left'][j], d['top'][j], d['width'][j], d['height'][j])\n",
    "        #         cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        #         cv2.rectangle(fragment, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            \n",
    "        #     # dla zapisu przykładowych detekcji odkomentowac ponizsze dwie linijki\n",
    "        #     #####\n",
    "        #     # cv2.imwrite(f\"tess_view_i{i}_k{k}.png\", img)\n",
    "        #     # cv2.imwrite(f\"wycinek_i{i}_k{k}.png\", fragment)\n",
    "        #     ####\n",
    "            \n",
    "np_us_wer = np.array(uwagi_scores)\n",
    "np_us_cer = np.array(uwagi_scores_cer)\n",
    "np_us_mer = np.array(uwagi_scores_mer)\n",
    "\n",
    "np_os_wer = np.array(odbiorca_scores)\n",
    "np_os_cer = np.array(odbiorca_scores_cer)\n",
    "np_os_mer = np.array(odbiorca_scores_mer)\n",
    "\n",
    "np_ns_wer = np.array(nadawca_scores)\n",
    "np_ns_cer = np.array(nadawca_scores_cer)\n",
    "np_ns_mer = np.array(nadawca_scores_mer)\n",
    "\n",
    "uwagi_verse_wer = {\n",
    "    \"kategoria\": \"Uwagi\",\n",
    "    \"WER mean\": np.mean(np_us_wer[:,0]),\n",
    "    \"WER std\": np.std(np_us_wer[:,0]),\n",
    "    \"Max WER mean\": np.mean(np_us_wer[:,1]),\n",
    "    \"Max WER std\": np.std(np_us_wer[:,1])\n",
    "}\n",
    "\n",
    "odbiorca_verse_wer = {\n",
    "    \"kategoria\": \"Odbiorca\",\n",
    "    \"WER mean\": np.mean(np_os_wer[:,0]),\n",
    "    \"WER std\": np.std(np_os_wer[:,0]),\n",
    "    \"Max WER mean\": np.mean(np_os_wer[:,1]),\n",
    "    \"Max WER std\": np.std(np_os_wer[:,1])\n",
    "}\n",
    "\n",
    "nadawca_verse_wer = {\n",
    "    \"kategoria\": \"Nadawca\",\n",
    "    \"WER mean\": np.mean(np_ns_wer[:,0]),\n",
    "    \"WER std\": np.std(np_ns_wer[:,0]),\n",
    "    \"Max WER mean\": np.mean(np_ns_wer[:,1]),\n",
    "    \"Max WER std\": np.std(np_ns_wer[:,1])\n",
    "}\n",
    "\n",
    "data = [uwagi_verse_wer, odbiorca_verse_wer, nadawca_verse_wer]\n",
    "cat_wer_df = pd.DataFrame(data)\n",
    "\n",
    "uwagi_verse_cer = {\n",
    "    \"kategoria\": \"Uwagi\",\n",
    "    \"CER mean\": np.mean(np_us_cer[:,0]),\n",
    "    \"CER std\": np.std(np_us_cer[:,0]),\n",
    "    \"Max CER mean\": np.mean(np_us_cer[:,1]),\n",
    "    \"Max CER std\": np.std(np_us_cer[:,1])\n",
    "}\n",
    "\n",
    "odbiorca_verse_cer = {\n",
    "    \"kategoria\": \"Odbiorca\",\n",
    "    \"CER mean\": np.mean(np_os_cer[:,0]),\n",
    "    \"CER std\": np.std(np_os_cer[:,0]),\n",
    "    \"Max CER mean\": np.mean(np_os_cer[:,1]),\n",
    "    \"Max CER std\": np.std(np_os_cer[:,1])\n",
    "}\n",
    "\n",
    "nadawca_verse_cer = {\n",
    "    \"kategoria\": \"Nadawca\",\n",
    "    \"CER mean\": np.mean(np_ns_cer[:,0]),\n",
    "    \"CER std\": np.std(np_ns_cer[:,0]),\n",
    "    \"Max CER mean\": np.mean(np_ns_cer[:,1]),\n",
    "    \"Max CER std\": np.std(np_ns_cer[:,1])\n",
    "}\n",
    "\n",
    "data = [uwagi_verse_cer, odbiorca_verse_cer, nadawca_verse_cer]\n",
    "cat_cer_df = pd.DataFrame(data)\n",
    "\n",
    "uwagi_verse_mer = {\n",
    "    \"kategoria\": \"Uwagi\",\n",
    "    \"MER mean\": np.mean(np_us_mer),\n",
    "    \"MER std\": np.std(np_us_mer)\n",
    "}\n",
    "\n",
    "odbiorca_verse_mer = {\n",
    "    \"kategoria\": \"Odbiorca\",\n",
    "    \"MER mean\": np.mean(np_os_mer),\n",
    "    \"MER std\": np.std(np_os_mer)\n",
    "}\n",
    "\n",
    "nadawca_verse_mer = {\n",
    "    \"kategoria\": \"Nadawca\",\n",
    "    \"MER mean\": np.mean(np_ns_mer),\n",
    "    \"MER std\": np.std(np_ns_mer)\n",
    "}\n",
    "\n",
    "data = [uwagi_verse_mer, odbiorca_verse_mer, nadawca_verse_mer]\n",
    "cat_mer_df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Miary dla detekcji zbioru treningowego, z podziałem na typ informacji: \\n\")\n",
    "# print(\"Uwagi WER:\", np.mean(uwagi_scores))\n",
    "# print(\"Nadawca WER:\", np.mean(nadawca_scores))\n",
    "# print(\"Odbiorca WER:\", np.mean(odbiorca_scores))\n",
    "# print(\"############\")\n",
    "# print(\"Uwagi CER:\", np.mean(uwagi_scores_cer))\n",
    "# print(\"Nadawca CER:\", np.mean(nadawca_scores_cer))\n",
    "# print(\"Odbiorca CER:\", np.mean(odbiorca_scores_cer))\n",
    "# print(\"############\")\n",
    "# print(\"Uwagi MER:\", np.mean(uwagi_scores_mer))\n",
    "# print(\"Nadawca MER:\", np.mean(nadawca_scores_mer))\n",
    "# print(\"Odbiorca MER:\", np.mean(odbiorca_scores_mer))\n",
    "print(cat_wer_df, \"\\n\")\n",
    "print(cat_cer_df, \"\\n\")\n",
    "print(cat_mer_df, \"\\n\")\n",
    "\n",
    "np_i = np.array(inpost_wer)\n",
    "np_p = np.array(pocztex_wer)\n",
    "np_dp = np.array(dpd_wer)\n",
    "np_dh = np.array(dhl_wer)\n",
    "\n",
    "i_verse = {\n",
    "    \"przewoznik\": \"inpost\",\n",
    "    \"WER mean\": np.mean(np_i[:,0]),\n",
    "    \"WER std\": np.std(np_i[:,0]),\n",
    "    \"Max WER mean\": np.mean(np_i[:,1]),\n",
    "    \"Max WER std\": np.std(np_i[:,1])\n",
    "}\n",
    "\n",
    "p_verse = {\n",
    "    \"przewoznik\": \"pocztex\",\n",
    "    \"WER mean\": np.mean(np_p[:,0]),\n",
    "    \"WER std\": np.std(np_p[:,0]),\n",
    "    \"Max WER mean\": np.mean(np_p[:,1]),\n",
    "    \"Max WER std\": np.std(np_p[:,1])\n",
    "}\n",
    "\n",
    "dp_verse = {\n",
    "    \"przewoznik\": \"dpd\",\n",
    "    \"WER mean\": np.mean(np_dp[:,0]),\n",
    "    \"WER std\": np.std(np_dp[:,0]),\n",
    "    \"Max WER mean\": np.mean(np_dp[:,1]),\n",
    "    \"Max WER std\": np.std(np_dp[:,1])\n",
    "}\n",
    "\n",
    "dh_verse = {\n",
    "    \"przewoznik\": \"dhl\",\n",
    "    \"WER mean\": np.mean(np_dh[:,0]),\n",
    "    \"WER std\": np.std(np_dh[:,0]),\n",
    "    \"Max WER mean\": np.mean(np_dh[:,1]),\n",
    "    \"Max WER std\": np.std(np_dh[:,1])\n",
    "}\n",
    "\n",
    "data = [i_verse, p_verse, dp_verse, dh_verse]\n",
    "prze_wer_df = pd.DataFrame(data)\n",
    "print(\"\\nMiary dla detekcji zbioru treningowego, z podziałem na typ przewoźnika:\")\n",
    "# print(\"Inpost WER: \", np.mean(inpost_wer))\n",
    "# print(\"Pocztex WER: \", np.mean(pocztex_wer))\n",
    "# print(\"DPD WER: \", np.mean(dpd_wer))\n",
    "# print(\"DHL WER: \", np.mean(dhl_wer))\n",
    "print(prze_wer_df, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miary dla detekcji zbioru walidacyjnego, z podziałem na typ informacji: \n",
      "\n",
      "  kategoria  WER mean   WER std  Max WER mean  Max WER std\n",
      "0     Uwagi  0.884211  0.268604      1.608772     0.258163\n",
      "1  Odbiorca  0.983367  0.444625      1.294764     0.463612\n",
      "2   Nadawca  0.857143  0.214297      1.088889     0.166508 \n",
      "\n",
      "  kategoria  CER mean   CER std  Max CER mean  Max CER std\n",
      "0     Uwagi  0.884211  0.268604      1.433874     0.269208\n",
      "1  Odbiorca  0.983367  0.444625      1.182904     0.382319\n",
      "2   Nadawca  0.857143  0.214297      1.037708     0.073272 \n",
      "\n",
      "  kategoria  MER mean   MER std\n",
      "0     Uwagi  0.736717  0.185487\n",
      "1  Odbiorca  0.793836  0.135512\n",
      "2   Nadawca  0.808071  0.189014 \n",
      "\n",
      "\n",
      "Miary dla detekcji zbioru walidacyjnego, z podziałem na typ przewoźnika:\n",
      "  przewoznik  WER mean   WER std  Max WER mean  Max WER std\n",
      "0     inpost  0.899216  0.241155      1.375670     0.366415\n",
      "1    pocztex  0.875309  0.146989      1.200000     0.205480\n",
      "2        dpd  1.276365  0.642310      1.738262     0.562352\n",
      "3        dhl  0.729012  0.186363      1.126543     0.165174 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# teraz obliczenia dla zbioru walidacyjnego\n",
    "\n",
    "class_names_dict = {0:\"nadawca\",\n",
    "                    1:\"odbiorca\",\n",
    "                    2:\"uwagi\"}\n",
    "\n",
    "nadawca_scores = []\n",
    "odbiorca_scores = []\n",
    "uwagi_scores = []\n",
    "\n",
    "nadawca_scores_cer = []\n",
    "odbiorca_scores_cer = []\n",
    "uwagi_scores_cer = []\n",
    "\n",
    "nadawca_scores_mer = []\n",
    "odbiorca_scores_mer = []\n",
    "uwagi_scores_mer = []\n",
    "\n",
    "inpost_wer = []\n",
    "dhl_wer = []\n",
    "dpd_wer = []\n",
    "pocztex_wer = []\n",
    "\n",
    "\n",
    "for k, result in enumerate(loaded_val_results):\n",
    "    orig_image = result.orig_img\n",
    "    fullpath = result.path\n",
    "    number = int(fullpath.split(\"_\")[-1][:-5]) # zadziała dla train i val, nie dla test\n",
    "    detected_classes = result.boxes.cls\n",
    "    detected_boxes = result.boxes\n",
    "    \n",
    "    for i, class_detected in enumerate(detected_classes):\n",
    "        class_int = int(class_detected.numpy())\n",
    "        [x1, y1, x2, y2] = detected_boxes[i].xyxy.numpy().astype(int)[0]\n",
    "        fragment = orig_image[y1:y2, x1:x2, :] # działa! \n",
    "        \n",
    "        # #blur\n",
    "        # img = cv2.blur(fragment, (10,10))\n",
    "        \n",
    "        # #sharpen\n",
    "        # kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "        # img = cv2.filter2D(img, -1, kernel)\n",
    "        \n",
    "        img = get_grayscale(fragment)\n",
    "        img = cv2.GaussianBlur(img,(5,5),0)\n",
    "        # img = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "        #     cv2.THRESH_BINARY,11,2)\n",
    "        img = thresholding(img)\n",
    "        \n",
    "        #open\n",
    "        kernel = np.ones((4,4),np.uint8)\n",
    "        img = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        #close\n",
    "        kernel = np.ones((4,4),np.uint8)\n",
    "        img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        \n",
    "        text_on_fragment = pytesseract.image_to_string(img)\n",
    "\n",
    "        text_category = class_names_dict[class_int]\n",
    "        text_to_decipher = generated_df.iloc[number][text_category]\n",
    "        label_type = generated_df.iloc[number]['label_type'].split('_')[0]\n",
    "        \n",
    "        \n",
    "        N_1 = len(re.split(r'\\s+', text_to_decipher))\n",
    "        N_2 = len(re.split(r'\\s+', text_on_fragment))\n",
    "        \n",
    "        max_wer = max(N_1, N_2)/N_1\n",
    "        \n",
    "        \n",
    "        wer = jiwer.wer(\n",
    "                text_to_decipher,\n",
    "                text_on_fragment,\n",
    "                truth_transform=transforms,\n",
    "                hypothesis_transform=transforms,\n",
    "            )\n",
    "\n",
    "        cleaned_str_ref = text_to_decipher.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        cleaned_str_hip = text_on_fragment.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        N_1 = len(cleaned_str_ref)\n",
    "        N_2 = len(cleaned_str_hip)\n",
    "        \n",
    "        max_cer = max(N_1, N_2)/N_1\n",
    "        \n",
    "        \n",
    "        cer = jiwer.cer(\n",
    "                text_to_decipher,\n",
    "                text_on_fragment,\n",
    "                truth_transform=transforms,\n",
    "                hypothesis_transform=transforms,\n",
    "            )\n",
    "        \n",
    "        mer = jiwer.mer(\n",
    "                text_to_decipher,\n",
    "                text_on_fragment,\n",
    "                truth_transform=transforms,\n",
    "                hypothesis_transform=transforms,\n",
    "            )\n",
    "        \n",
    "        \n",
    "        if label_type == \"inpost\":\n",
    "            inpost_wer.append([wer, max_wer])\n",
    "        elif label_type == \"dhl\":\n",
    "            dhl_wer.append([wer, max_wer])\n",
    "        elif label_type == \"pocztex\":\n",
    "            pocztex_wer.append([wer, max_wer])\n",
    "        else:\n",
    "            dpd_wer.append([wer, max_wer])\n",
    "        \n",
    "        if class_int == 0:\n",
    "            nadawca_scores.append([wer, max_wer])\n",
    "            nadawca_scores_cer.append([cer, max_cer])\n",
    "            nadawca_scores_mer.append(mer)\n",
    "        elif class_int == 1:\n",
    "            odbiorca_scores.append([wer, max_wer])\n",
    "            odbiorca_scores_cer.append([cer, max_cer])\n",
    "            odbiorca_scores_mer.append(mer)\n",
    "        elif class_int == 2:\n",
    "            uwagi_scores.append([wer, max_wer])\n",
    "            uwagi_scores_cer.append([cer, max_cer])\n",
    "            uwagi_scores_mer.append(mer)\n",
    "        \n",
    "        # cały fragment w dół jedynie dla wglądu w wyniki w razie potrzeby\n",
    "        # if k in [4, 15, 17]:\n",
    "            \n",
    "        #     # dla wglądu w to co model wykrywa (tekst) i porównania co powinno wykryć, odkomentować ponizsza linijke\n",
    "        #     #####\n",
    "        #     # print(\"Co wykryło: \\n ######### \\n\", text_on_fragment, \"\\n #########, co miało być: ######### \\n\", text_to_decipher, \"\\n\\n\")\n",
    "        #     #####\n",
    "            \n",
    "        #     # umieszczanie bounding boxów na obrazach\n",
    "        #     d = pytesseract.image_to_data(img, output_type=Output.DICT)\n",
    "        #     n_boxes = len(d['level'])\n",
    "        #     for j in range(n_boxes):\n",
    "        #         (x, y, w, h) = (d['left'][j], d['top'][j], d['width'][j], d['height'][j])\n",
    "        #         cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        #         cv2.rectangle(fragment, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            \n",
    "        #     # dla zapisu przykładowych detekcji odkomentowac ponizsze dwie linijki\n",
    "        #     #####\n",
    "        #     # cv2.imwrite(f\"tess_view_i{i}_k{k}.png\", img)\n",
    "        #     # cv2.imwrite(f\"wycinek_i{i}_k{k}.png\", fragment)\n",
    "        #     ####\n",
    "            \n",
    "np_us_wer = np.array(uwagi_scores)\n",
    "np_us_cer = np.array(uwagi_scores_cer)\n",
    "np_us_mer = np.array(uwagi_scores_mer)\n",
    "\n",
    "np_os_wer = np.array(odbiorca_scores)\n",
    "np_os_cer = np.array(odbiorca_scores_cer)\n",
    "np_os_mer = np.array(odbiorca_scores_mer)\n",
    "\n",
    "np_ns_wer = np.array(nadawca_scores)\n",
    "np_ns_cer = np.array(nadawca_scores_cer)\n",
    "np_ns_mer = np.array(nadawca_scores_mer)\n",
    "\n",
    "uwagi_verse_wer = {\n",
    "    \"kategoria\": \"Uwagi\",\n",
    "    \"WER mean\": np.mean(np_us_wer[:,0]),\n",
    "    \"WER std\": np.std(np_us_wer[:,0]),\n",
    "    \"Max WER mean\": np.mean(np_us_wer[:,1]),\n",
    "    \"Max WER std\": np.std(np_us_wer[:,1])\n",
    "}\n",
    "\n",
    "odbiorca_verse_wer = {\n",
    "    \"kategoria\": \"Odbiorca\",\n",
    "    \"WER mean\": np.mean(np_os_wer[:,0]),\n",
    "    \"WER std\": np.std(np_os_wer[:,0]),\n",
    "    \"Max WER mean\": np.mean(np_os_wer[:,1]),\n",
    "    \"Max WER std\": np.std(np_os_wer[:,1])\n",
    "}\n",
    "\n",
    "nadawca_verse_wer = {\n",
    "    \"kategoria\": \"Nadawca\",\n",
    "    \"WER mean\": np.mean(np_ns_wer[:,0]),\n",
    "    \"WER std\": np.std(np_ns_wer[:,0]),\n",
    "    \"Max WER mean\": np.mean(np_ns_wer[:,1]),\n",
    "    \"Max WER std\": np.std(np_ns_wer[:,1])\n",
    "}\n",
    "\n",
    "data = [uwagi_verse_wer, odbiorca_verse_wer, nadawca_verse_wer]\n",
    "cat_wer_df = pd.DataFrame(data)\n",
    "\n",
    "uwagi_verse_cer = {\n",
    "    \"kategoria\": \"Uwagi\",\n",
    "    \"CER mean\": np.mean(np_us_cer[:,0]),\n",
    "    \"CER std\": np.std(np_us_cer[:,0]),\n",
    "    \"Max CER mean\": np.mean(np_us_cer[:,1]),\n",
    "    \"Max CER std\": np.std(np_us_cer[:,1])\n",
    "}\n",
    "\n",
    "odbiorca_verse_cer = {\n",
    "    \"kategoria\": \"Odbiorca\",\n",
    "    \"CER mean\": np.mean(np_os_cer[:,0]),\n",
    "    \"CER std\": np.std(np_os_cer[:,0]),\n",
    "    \"Max CER mean\": np.mean(np_os_cer[:,1]),\n",
    "    \"Max CER std\": np.std(np_os_cer[:,1])\n",
    "}\n",
    "\n",
    "nadawca_verse_cer = {\n",
    "    \"kategoria\": \"Nadawca\",\n",
    "    \"CER mean\": np.mean(np_ns_cer[:,0]),\n",
    "    \"CER std\": np.std(np_ns_cer[:,0]),\n",
    "    \"Max CER mean\": np.mean(np_ns_cer[:,1]),\n",
    "    \"Max CER std\": np.std(np_ns_cer[:,1])\n",
    "}\n",
    "\n",
    "data = [uwagi_verse_cer, odbiorca_verse_cer, nadawca_verse_cer]\n",
    "cat_cer_df = pd.DataFrame(data)\n",
    "\n",
    "uwagi_verse_mer = {\n",
    "    \"kategoria\": \"Uwagi\",\n",
    "    \"MER mean\": np.mean(np_us_mer),\n",
    "    \"MER std\": np.std(np_us_mer)\n",
    "}\n",
    "\n",
    "odbiorca_verse_mer = {\n",
    "    \"kategoria\": \"Odbiorca\",\n",
    "    \"MER mean\": np.mean(np_os_mer),\n",
    "    \"MER std\": np.std(np_os_mer)\n",
    "}\n",
    "\n",
    "nadawca_verse_mer = {\n",
    "    \"kategoria\": \"Nadawca\",\n",
    "    \"MER mean\": np.mean(np_ns_mer),\n",
    "    \"MER std\": np.std(np_ns_mer)\n",
    "}\n",
    "\n",
    "data = [uwagi_verse_mer, odbiorca_verse_mer, nadawca_verse_mer]\n",
    "cat_mer_df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Miary dla detekcji zbioru walidacyjnego, z podziałem na typ informacji: \\n\")\n",
    "# print(\"Uwagi WER:\", np.mean(uwagi_scores))\n",
    "# print(\"Nadawca WER:\", np.mean(nadawca_scores))\n",
    "# print(\"Odbiorca WER:\", np.mean(odbiorca_scores))\n",
    "# print(\"############\")\n",
    "# print(\"Uwagi CER:\", np.mean(uwagi_scores_cer))\n",
    "# print(\"Nadawca CER:\", np.mean(nadawca_scores_cer))\n",
    "# print(\"Odbiorca CER:\", np.mean(odbiorca_scores_cer))\n",
    "# print(\"############\")\n",
    "# print(\"Uwagi MER:\", np.mean(uwagi_scores_mer))\n",
    "# print(\"Nadawca MER:\", np.mean(nadawca_scores_mer))\n",
    "# print(\"Odbiorca MER:\", np.mean(odbiorca_scores_mer))\n",
    "print(cat_wer_df, \"\\n\")\n",
    "print(cat_cer_df, \"\\n\")\n",
    "print(cat_mer_df, \"\\n\")\n",
    "\n",
    "np_i = np.array(inpost_wer)\n",
    "np_p = np.array(pocztex_wer)\n",
    "np_dp = np.array(dpd_wer)\n",
    "np_dh = np.array(dhl_wer)\n",
    "\n",
    "i_verse = {\n",
    "    \"przewoznik\": \"inpost\",\n",
    "    \"WER mean\": np.mean(np_i[:,0]),\n",
    "    \"WER std\": np.std(np_i[:,0]),\n",
    "    \"Max WER mean\": np.mean(np_i[:,1]),\n",
    "    \"Max WER std\": np.std(np_i[:,1])\n",
    "}\n",
    "\n",
    "p_verse = {\n",
    "    \"przewoznik\": \"pocztex\",\n",
    "    \"WER mean\": np.mean(np_p[:,0]),\n",
    "    \"WER std\": np.std(np_p[:,0]),\n",
    "    \"Max WER mean\": np.mean(np_p[:,1]),\n",
    "    \"Max WER std\": np.std(np_p[:,1])\n",
    "}\n",
    "\n",
    "dp_verse = {\n",
    "    \"przewoznik\": \"dpd\",\n",
    "    \"WER mean\": np.mean(np_dp[:,0]),\n",
    "    \"WER std\": np.std(np_dp[:,0]),\n",
    "    \"Max WER mean\": np.mean(np_dp[:,1]),\n",
    "    \"Max WER std\": np.std(np_dp[:,1])\n",
    "}\n",
    "\n",
    "dh_verse = {\n",
    "    \"przewoznik\": \"dhl\",\n",
    "    \"WER mean\": np.mean(np_dh[:,0]),\n",
    "    \"WER std\": np.std(np_dh[:,0]),\n",
    "    \"Max WER mean\": np.mean(np_dh[:,1]),\n",
    "    \"Max WER std\": np.std(np_dh[:,1])\n",
    "}\n",
    "\n",
    "data = [i_verse, p_verse, dp_verse, dh_verse]\n",
    "prze_wer_df = pd.DataFrame(data)\n",
    "print(\"\\nMiary dla detekcji zbioru walidacyjnego, z podziałem na typ przewoźnika:\")\n",
    "# print(\"Inpost WER: \", np.mean(inpost_wer))\n",
    "# print(\"Pocztex WER: \", np.mean(pocztex_wer))\n",
    "# print(\"DPD WER: \", np.mean(dpd_wer))\n",
    "# print(\"DHL WER: \", np.mean(dhl_wer))\n",
    "print(prze_wer_df, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
